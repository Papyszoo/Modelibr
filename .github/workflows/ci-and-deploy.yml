name: CI and Deploy Docs

on:
  push:
    branches: ['**']
  pull_request:
    branches: ['**']
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pages: write
  id-token: write
  actions: read

jobs:
  backend-tests:
    name: Backend Unit Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET 9
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '9.0.x'

      - name: Restore dependencies
        run: dotnet restore Modelibr.sln

      - name: Build
        run: dotnet build Modelibr.sln --no-restore

      - name: Run tests
        run: dotnet test Modelibr.sln --no-build --verbosity normal --logger "trx;LogFileName=test-results.trx"

      - name: Parse test results
        if: always()
        run: |
          # Find and parse TRX files from all test projects
          total=0
          passed=0
          failed=0
          failures="[]"

          # Find all TRX files in TestResults directories (recursively under tests/)
          trx_files=$(find tests -type d -name "TestResults" -exec find {} -type f -name "test-results.trx" \; 2>/dev/null || true)

          if [ -n "$trx_files" ]; then
            echo "Found TRX files:"
            echo "$trx_files"

            # Sum up test results from all TRX files
            while IFS= read -r trx_file; do
              if [ -f "$trx_file" ]; then
                file_total=$(grep -o 'total="[0-9]*"' "$trx_file" | grep -o '[0-9]*' || echo "0")
                file_passed=$(grep -o 'passed="[0-9]*"' "$trx_file" | grep -o '[0-9]*' || echo "0")
                file_failed=$(grep -o 'failed="[0-9]*"' "$trx_file" | grep -o '[0-9]*' || echo "0")

                total=$((total + file_total))
                passed=$((passed + file_passed))
                failed=$((failed + file_failed))
                
                # Extract failed test names and messages from TRX
                if [ "$file_failed" -gt 0 ]; then
                  # Parse failed tests - TRX format has UnitTestResult with outcome="Failed"
                  failed_tests=$(grep -oP 'testName="[^"]+"[^>]*outcome="Failed"' "$trx_file" | grep -oP 'testName="\K[^"]+' || true)
                  if [ -n "$failed_tests" ]; then
                    while IFS= read -r test_name; do
                      # Try to get error message (simplified - may not get full message)
                      error_msg=$(grep -A5 "testName=\"$test_name\"" "$trx_file" | grep -oP '<Message>\K[^<]+' | head -1 || echo "Test failed")
                      error_msg=$(echo "$error_msg" | sed 's/"/\\"/g' | tr '\n' ' ' | cut -c1-200)
                      failures=$(echo "$failures" | jq --arg name "$test_name" --arg msg "$error_msg" '. + [{"name": $name, "message": $msg}]')
                    done <<< "$failed_tests"
                  fi
                fi
              fi
            done <<< "$trx_files"

            # Create JSON summary
            mkdir -p test-results
            echo "{\"total\": ${total}, \"passed\": ${passed}, \"failed\": ${failed}, \"framework\": \".NET 9.0\", \"failures\": ${failures}}" > test-results/backend-results.json
            echo "Backend tests: ${passed}/${total} passed, ${failed} failed"
          else
            echo "No test results found"
            mkdir -p test-results
            echo '{"total": 0, "passed": 0, "failed": 0, "framework": ".NET 9.0", "failures": [], "error": "No results"}' > test-results/backend-results.json
          fi

      - name: Upload backend test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: backend-test-results-${{ github.run_number }}
          path: test-results/backend-results.json
          retention-days: 30

  frontend-tests:
    name: Frontend Unit Tests
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: src/frontend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: src/frontend/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: Run tests
        id: frontend-test
        continue-on-error: true
        run: npm test 2>&1 | tee test-output.txt

      - name: Parse test results
        if: always()
        working-directory: src/frontend
        run: |
          # Parse Jest output from console
          if [ -f "test-output.txt" ]; then
            # Extract test counts from Jest summary output using sed
            # Jest outputs something like "Tests:       5 passed, 5 total"
            total=$(sed -n 's/.*Tests:.*\([0-9]\+\) total.*/\1/p' test-output.txt | tail -1)
            passed=$(sed -n 's/.*Tests:.*\([0-9]\+\) passed.*/\1/p' test-output.txt | tail -1)
            failed=$(sed -n 's/.*Tests:.*\([0-9]\+\) failed.*/\1/p' test-output.txt | tail -1)

            # Set defaults if empty
            total=${total:-0}
            passed=${passed:-0}
            failed=${failed:-0}

            # If total is 0 but we have passed tests, total = passed + failed
            if [ "$total" = "0" ] && [ "$passed" != "0" ]; then
              total=$((passed + failed))
            fi

            # Extract failing test names and messages from Jest output
            # Jest format: "● TestSuiteName › test name" followed by error
            failures="[]"
            if [ "$failed" -gt 0 ]; then
              # Extract failed test blocks (lines starting with ●)
              grep -n "^[[:space:]]*●" test-output.txt | while IFS=: read -r line_num line_content; do
                test_name=$(echo "$line_content" | sed 's/^[[:space:]]*● //' | cut -c1-150)
                # Get error message from next few lines
                error_msg=$(sed -n "$((line_num+1)),$((line_num+5))p" test-output.txt | tr '\n' ' ' | sed 's/"/\\"/g' | cut -c1-200)
                failures=$(echo "$failures" | jq --arg name "$test_name" --arg msg "$error_msg" '. + [{"name": $name, "message": $msg}]')
              done 2>/dev/null || true
            fi
            
            # Handle case where jq parsing failed
            if ! echo "$failures" | jq . >/dev/null 2>&1; then
              failures="[]"
            fi

            # Create summary JSON
            mkdir -p "$GITHUB_WORKSPACE/test-results"
            echo "{\"total\": ${total}, \"passed\": ${passed}, \"failed\": ${failed}, \"framework\": \"Jest\", \"failures\": ${failures}}" > "$GITHUB_WORKSPACE/test-results/frontend-results.json"
            echo "Frontend tests: ${passed}/${total} passed, ${failed} failed"
          else
            echo "No test output found"
            mkdir -p "$GITHUB_WORKSPACE/test-results"
            echo '{"total": 0, "passed": 0, "failed": 0, "framework": "Jest", "failures": [], "error": "No results"}' > "$GITHUB_WORKSPACE/test-results/frontend-results.json"
          fi

      - name: Upload frontend test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: frontend-test-results-${{ github.run_number }}
          path: test-results/frontend-results.json
          retention-days: 30

  blender-addon-tests:
    name: Blender Addon Tests
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: blender-addon

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: blender-addon/tests/

      - name: Install test dependencies
        run: |
          pip install pytest fake-bpy-module-latest numpy

      - name: Run unit tests
        id: blender-test
        continue-on-error: true
        run: python -m pytest tests/unit -v 2>&1 | tee test-output.txt

      - name: Parse test results
        if: always()
        working-directory: blender-addon
        run: |
          # Parse pytest output from console
          if [ -f "test-output.txt" ]; then
            # Extract test counts from pytest summary output using sed
            # pytest outputs something like "5 passed, 1 failed in 2.5s"
            passed=$(sed -n 's/.*\([0-9]\+\) passed.*/\1/p' test-output.txt | tail -1)
            failed=$(sed -n 's/.*\([0-9]\+\) failed.*/\1/p' test-output.txt | tail -1)

            # Set defaults if empty
            passed=${passed:-0}
            failed=${failed:-0}
            total=$((passed + failed))

            # If we didn't find any results, try alternative patterns
            if [ "$total" = "0" ]; then
              # Try to find the total from collected tests
              collected=$(sed -n 's/.*collected \([0-9]\+\).*/\1/p' test-output.txt | head -1)
              collected=${collected:-0}
              if [ "$collected" != "0" ]; then
                total=$collected
                # Check if all passed (no failed mentioned)
                if [ "$failed" = "0" ] && grep -q "passed" test-output.txt; then
                  passed=$total
                fi
              fi
            fi

            # Extract failing test names and messages from pytest output
            # pytest format: "FAILED tests/path::test_name - Error message"
            failures="[]"
            if [ "$failed" -gt 0 ]; then
              grep "^FAILED" test-output.txt | while IFS= read -r line; do
                test_name=$(echo "$line" | sed 's/^FAILED //' | cut -d' ' -f1 | cut -c1-150)
                error_msg=$(echo "$line" | sed 's/^FAILED [^ ]* - //' | sed 's/"/\\"/g' | cut -c1-200)
                failures=$(echo "$failures" | jq --arg name "$test_name" --arg msg "$error_msg" '. + [{"name": $name, "message": $msg}]')
              done 2>/dev/null || true
            fi
            
            # Handle case where jq parsing failed
            if ! echo "$failures" | jq . >/dev/null 2>&1; then
              failures="[]"
            fi

            # Create summary JSON
            mkdir -p "$GITHUB_WORKSPACE/test-results"
            echo "{\"total\": ${total}, \"passed\": ${passed}, \"failed\": ${failed}, \"framework\": \"pytest\", \"failures\": ${failures}}" > "$GITHUB_WORKSPACE/test-results/blender-results.json"
            echo "Blender addon tests: ${passed}/${total} passed, ${failed} failed"
          else
            echo "No test output found"
            mkdir -p "$GITHUB_WORKSPACE/test-results"
            echo '{"total": 0, "passed": 0, "failed": 0, "framework": "pytest", "failures": [], "error": "No results"}' > "$GITHUB_WORKSPACE/test-results/blender-results.json"
          fi

      - name: Upload blender test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: blender-test-results-${{ github.run_number }}
          path: test-results/blender-results.json
          retention-days: 30

      - name: Install Blender
        run: |
          sudo snap install blender --classic
          blender --version

      - name: Run Blender integration tests
        uses: GabrielBB/xvfb-action@v1
        with:
          working-directory: blender-addon
          run: blender -b --python tests/integration/run_in_blender.py

  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build and start services
        run: |
          docker compose -f tests/e2e/docker-compose.e2e.yml up -d --build

      - name: Show container status
        if: always()
        run: |
          echo "=== Container status ==="
          docker compose -f tests/e2e/docker-compose.e2e.yml ps -a
          echo ""
          echo "=== webapi-e2e logs ==="
          docker compose -f tests/e2e/docker-compose.e2e.yml logs webapi-e2e 2>&1 | tail -100 || true
          echo ""
          echo "=== postgres-e2e logs ==="
          docker compose -f tests/e2e/docker-compose.e2e.yml logs postgres-e2e 2>&1 | tail -50 || true

      - name: Wait for services to be ready
        run: |
          echo "Waiting for services to be healthy..."
          timeout 120 bash -c 'until curl -sf http://localhost:8090/health; do echo "Waiting for backend..."; sleep 5; done'
          echo "Backend is ready!"
          timeout 60 bash -c 'until curl -sf http://localhost:3002; do echo "Waiting for frontend..."; sleep 5; done'
          echo "Frontend is ready!"

      - name: Setup Node.js for E2E
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: tests/e2e/package-lock.json

      - name: Install E2E dependencies
        working-directory: tests/e2e
        run: npm ci

      - name: Install Playwright browsers
        working-directory: tests/e2e
        run: npx playwright install --with-deps chromium

      - name: Run E2E tests
        id: e2e
        continue-on-error: true
        working-directory: tests/e2e
        run: npm run test:quick
        env:
          FRONTEND_URL: http://localhost:3002

      - name: Upload Playwright report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report-${{ github.run_number }}
          path: tests/e2e/playwright-report/
          retention-days: 30
          if-no-files-found: ignore

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ github.run_number }}
          path: tests/e2e/test-results/
          retention-days: 30
          if-no-files-found: ignore

      - name: Stop services
        if: always()
        run: |
          echo "=== Final container status ==="
          docker compose -f tests/e2e/docker-compose.e2e.yml ps -a || true
          echo ""
          echo "=== webapi-e2e logs (full) ==="
          docker compose -f tests/e2e/docker-compose.e2e.yml logs webapi-e2e 2>&1 || true
          echo ""
          echo "=== frontend-e2e logs ==="
          docker compose -f tests/e2e/docker-compose.e2e.yml logs frontend-e2e 2>&1 | tail -50 || true
          echo ""
          docker compose -f tests/e2e/docker-compose.e2e.yml down -v

      - name: Fail if E2E tests failed
        if: steps.e2e.outcome == 'failure'
        run: exit 1

  ci-status:
    name: CI Status
    runs-on: ubuntu-latest
    needs: [backend-tests, frontend-tests, blender-addon-tests, e2e-tests]
    if: always()

    steps:
      - name: Check all jobs succeeded
        if: contains(needs.*.result, 'failure') || contains(needs.*.result, 'cancelled')
        run: |
          echo "CI checks failed!"
          exit 1

      - name: All checks passed
        run: |
          echo "All CI checks passed!"

  build-docs:
    name: Build Documentation with Reports
    runs-on: ubuntu-latest
    needs: [e2e-tests]
    # Always run to generate reports, even if tests fail
    if: always()

    steps:
      - name: Checkout current branch for scripts
        uses: actions/checkout@v4
        with:
          path: current-branch

      - name: Checkout main branch for docs
        uses: actions/checkout@v4
        with:
          ref: main
          path: main-branch

      - name: Copy scripts from current branch
        run: |
          mkdir -p .github/scripts
          cp -r current-branch/.github/scripts/* .github/scripts/ || true

      - name: Copy docs from main branch
        run: |
          cp -r main-branch/docs ./docs

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm
          cache-dependency-path: docs/package-lock.json

      - name: Fetch last 10 test reports from all branches
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY_OWNER: ${{ github.repository_owner }}
        run: |
          chmod +x .github/scripts/fetch-test-reports.sh
          .github/scripts/fetch-test-reports.sh

      - name: Install dependencies
        working-directory: docs
        run: npm ci

      - name: Build website
        working-directory: docs
        run: npm run build

      - name: Upload docs with reports (for preview)
        uses: actions/upload-artifact@v4
        with:
          name: docs-with-reports
          path: docs/build/
          retention-days: 30

      - name: Upload artifact for GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs/build

  deploy-docs:
    name: Deploy to GitHub Pages
    runs-on: ubuntu-latest
    needs: [build-docs]
    # Always deploy after docs are built
    if: always() && needs.build-docs.result == 'success'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
